{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "o5famy7lur8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies installed successfully\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    subprocess.run(\n",
    "        [\"uv\", \"pip\", \"install\", \"-r\", \"requirements.txt\"],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        check=True\n",
    "    )\n",
    "    print(\"Dependencies installed successfully\")\n",
    "except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "    # Fallback to pip if uv is not available\n",
    "    subprocess.run(\n",
    "        [sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"requirements.txt\"],\n",
    "        check=True\n",
    "    )\n",
    "    print(\"Dependencies installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "v34dpxd5699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ghyykmt1p3u",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Firecrawl API Key: Set\n",
      "RunPod Endpoint: Set\n"
     ]
    }
   ],
   "source": [
    "# Environment variables\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Load from .env file if it exists\n",
    "env_file = Path(\".env\")\n",
    "if env_file.exists():\n",
    "    with open(env_file) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('#') and '=' in line:\n",
    "                key, value = line.split('=', 1)\n",
    "                os.environ[key] = value\n",
    "\n",
    "# RunPod Pod endpoint (get from: https://www.runpod.io/console/pods)\n",
    "# Deploy a pod with vLLM template and GPT-OSS 20B model\n",
    "RUNPOD_ENDPOINT_URL = os.getenv(\"RUNPOD_ENDPOINT_URL\", \"YOUR_RUNPOD_ENDPOINT_HERE\")\n",
    "RUNPOD_API_KEY = os.getenv(\"RUNPOD_API_KEY\", \"YOUR_RUNPOD_API_KEY_HERE\")\n",
    "\n",
    "# Firecrawl API (get from: https://www.firecrawl.dev/)\n",
    "FIRECRAWL_API_KEY = os.getenv(\"FIRECRAWL_API_KEY\", \"YOUR_FIRECRAWL_API_KEY_HERE\")\n",
    "\n",
    "print(f\"Firecrawl API Key: {'Set' if FIRECRAWL_API_KEY != 'YOUR_FIRECRAWL_API_KEY_HERE' else 'Not set'}\")\n",
    "print(f\"RunPod Endpoint: {'Set' if RUNPOD_ENDPOINT_URL != 'YOUR_RUNPOD_ENDPOINT_HERE' else 'Not set'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ju945kyosq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Firecrawl MCP server\n",
      "Available tools: ['firecrawl_scrape', 'firecrawl_map', 'firecrawl_search', 'firecrawl_crawl', 'firecrawl_check_crawl_status', 'firecrawl_extract']\n"
     ]
    }
   ],
   "source": [
    "# Connect to Firecrawl MCP Server using LangChain MCP Adapters\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "# Configure Firecrawl MCP server\n",
    "mcp_client = MultiServerMCPClient({\n",
    "    \"firecrawl\": {\n",
    "        \"transport\": \"stdio\",\n",
    "        \"command\": \"npx\",\n",
    "        \"args\": [\"-y\", \"firecrawl-mcp\"],\n",
    "        \"env\": {\n",
    "            **os.environ,\n",
    "            \"FIRECRAWL_API_KEY\": FIRECRAWL_API_KEY\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "# Get tools from MCP server\n",
    "# Note: Jupyter notebooks have a running event loop, so we use await directly\n",
    "firecrawl_tools = await mcp_client.get_tools()\n",
    "\n",
    "print(f\"Connected to Firecrawl MCP server\")\n",
    "print(f\"Available tools: {[tool.name for tool in firecrawl_tools]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0831he4e27ua",
   "metadata": {},
   "source": [
    "# AI Job Application Assistant Workshop\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this workshop, you'll build an intelligent agent that helps students find jobs, evaluate their fit, and write personalized cover letters.\n",
    "\n",
    "### What You'll Learn\n",
    "- How LLM agents make decisions\n",
    "- vLLM for fast, local inference\n",
    "- LangChain v1 agent architecture\n",
    "- Multi-step reasoning with tools\n",
    "- Real-time web scraping with MCP\n",
    "- Production-ready integrations\n",
    "\n",
    "### The Problem\n",
    "Students manually apply to dozens of jobs:\n",
    "- Copy-pasting job descriptions\n",
    "- Rewriting cover letters for each role\n",
    "- Uncertain if they're qualified\n",
    "- No systematic approach\n",
    "\n",
    "### Our Solution\n",
    "An agent that autonomously:\n",
    "1. **Finds real jobs** - Scrapes live job boards using Firecrawl MCP\n",
    "2. **Scores resume fit** - Analyzes skills match\n",
    "3. **Generates personalized cover letters** - Tailored to each role\n",
    "4. **Makes intelligent tool selections** - ReAct-style reasoning\n",
    "\n",
    "### Architecture\n",
    "```\n",
    "User Query → vLLM Agent → Tool Selection → Response\n",
    "                   ↓\n",
    "        [Job Search (Firecrawl MCP) | Resume Scorer | Cover Letter]\n",
    "```\n",
    "\n",
    "### Key Technologies\n",
    "- **LangChain v1** - Modern agent framework with `create_agent()`\n",
    "- **vLLM** - Fast LLM inference on GPUs\n",
    "- **Hermes-2-Pro-Mistral-7B** - Excellent tool calling support\n",
    "- **Firecrawl MCP** - Production web scraping via Model Context Protocol\n",
    "- **RunPod** - GPU infrastructure for vLLM\n",
    "\n",
    "**This is a realistic end-to-end demo with no mock data!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0zbpko23j2a",
   "metadata": {},
   "source": [
    "## Step 1: Setup vLLM\n",
    "\n",
    "We'll use vLLM running on a RunPod Pod with PyTorch 2.4.0 template.\n",
    "\n",
    "**Model Used**: Hermes-2-Pro-Mistral-7B (excellent tool calling support)\n",
    "\n",
    "**Setup Steps**:\n",
    "1. Deploy RunPod pod with PyTorch 2.4.0 template (50GB+ container disk)\n",
    "2. SSH into pod and run:\n",
    "   ```bash\n",
    "   pip install vllm requests\n",
    "   python -m vllm.entrypoints.openai.api_server \\\n",
    "       --model NousResearch/Hermes-2-Pro-Mistral-7B \\\n",
    "       --host 0.0.0.0 \\\n",
    "       --port 8000 \\\n",
    "       --trust-remote-code \\\n",
    "       --enable-auto-tool-choice \\\n",
    "       --tool-call-parser hermes\n",
    "   ```\n",
    "3. Copy endpoint URL (https://[pod-id]-8000.proxy.runpod.net) to .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "x8tjvc75lfj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to RunPod pod\n",
      "Test response: \"Hello from vLLM! Wishing you warm greetings and a fantastic day ahead!\"\n"
     ]
    }
   ],
   "source": [
    "# Initialize vLLM on RunPod Pod\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import SecretStr\n",
    "\n",
    "if RUNPOD_ENDPOINT_URL == \"YOUR_RUNPOD_ENDPOINT_HERE\":\n",
    "    raise ValueError(\n",
    "        \"RunPod endpoint not configured. Please:\\n\"\n",
    "        \"1. Deploy a RunPod pod with PyTorch template\\n\"\n",
    "        \"2. Install vLLM and start server with Hermes-2-Pro-Mistral-7B\\n\"\n",
    "        \"3. Add RUNPOD_ENDPOINT_URL and RUNPOD_API_KEY to .env file\"\n",
    "    )\n",
    "\n",
    "# LangChain v1 / Modern OpenAI SDK parameters\n",
    "llm = ChatOpenAI(\n",
    "    api_key=SecretStr(RUNPOD_API_KEY),\n",
    "    base_url=RUNPOD_ENDPOINT_URL,\n",
    "    model=\"NousResearch/Hermes-2-Pro-Mistral-7B\",\n",
    "    max_completion_tokens=512,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Connected to RunPod pod\")\n",
    "\n",
    "# Test the connection\n",
    "try:\n",
    "    response = llm.invoke(\"Say 'Hello from vLLM!' in one sentence.\")\n",
    "    print(f\"Test response: {response.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Connection test failed: {e}\")\n",
    "    print(\"The model may still be loading. Wait a few minutes and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fx3a840cmlh",
   "metadata": {},
   "source": [
    "## Step 2: Connect to MCP Tools\n",
    "\n",
    "We've connected to Firecrawl MCP server to get web scraping tools. The agent will use these to fetch job listings from real websites.\n",
    "\n",
    "**Key Point:** The agent receives tools directly and decides when/how to use them based on the system prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lwwygtmsjxb",
   "metadata": {},
   "source": [
    "## Step 3: Build the Agent with LangChain v1\n",
    "\n",
    "Now we create the agent using **LangChain v1's modern architecture**. The agent will:\n",
    "1. Receive Firecrawl MCP tools directly (no wrappers!)\n",
    "2. Use its reasoning to decide when to scrape job sites\n",
    "3. Construct appropriate URLs based on system prompt guidance\n",
    "4. Parse and analyze scraped content\n",
    "5. Score resumes and write cover letters using its native language abilities\n",
    "\n",
    "**LangChain v1 Architecture:**\n",
    "- Uses `create_agent()` - simple, clean API\n",
    "- Built on LangGraph for durable execution and streaming\n",
    "- Message-based invocation: `agent.invoke({\"messages\": [...]})`\n",
    "- System prompt guides tool usage\n",
    "\n",
    "**Key Insight:**\n",
    "The LLM agent is intelligent enough to:\n",
    "- Construct job site URLs (e.g., naukri.com/python-developer-jobs-in-bangalore)\n",
    "- Use firecrawl_scrape appropriately\n",
    "- Parse markdown results\n",
    "- Compare skills and write cover letters without separate tools\n",
    "\n",
    "This demonstrates **true agentic AI** - the agent reasons about how to use tools, not just executing predefined wrappers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5legd0lb544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent created successfully!\n",
      "Tools available: 6\n",
      "\n",
      "Agent test response:\n",
      "I can assist you with the following tasks:\n",
      "\n",
      "1. Job searching: Using Firecrawl tools, I can scrape job listings from websites like Naukri.com (for Indian jobs) and LinkedIn. You can provide the role and location you're interested in, and I'll find relevant job openings.\n",
      "\n",
      "2. Resume scoring: If you have a list of skills for a particular job or role, I can compare them with the job requirements and provide a score on how well the candidate's skills match the job requirements.\n",
      "\n",
      "3. Cover letters: If you need help in crafting a cover letter, I can generate a personalized cover letter using my language abilities, based on the job description and your experience.\n",
      "\n",
      "Please let me know which of these tasks you'd like me to help with, and provide any necessary details.\n"
     ]
    }
   ],
   "source": [
    "# Create the agent using LangChain v1\n",
    "\n",
    "# System prompt guides the agent on how to use tools\n",
    "system_prompt = \"\"\"You are an intelligent job search assistant helping students find and apply to jobs.\n",
    "\n",
    "You have access to Firecrawl MCP tools for web scraping:\n",
    "- firecrawl_scrape: Scrape a single URL and extract content as markdown\n",
    "- firecrawl_search: Search for jobs using the keywords\n",
    "\n",
    "Common job sites and URL patterns:\n",
    "- Naukri.com (India): https://www.naukri.com/{role-with-dashes}-jobs-in-{location}\n",
    "  Example: https://www.naukri.com/python-developer-jobs-in-bangalore\n",
    "- LinkedIn: https://www.linkedin.com/jobs/search/?keywords={role}&location={location}\n",
    "\n",
    "When asked to find jobs:\n",
    "1. Construct the appropriate job site URL\n",
    "2. Use firecrawl_scrape to get job listings\n",
    "3. Analyze the scraped content and present top matches\n",
    "\n",
    "You can also help with:\n",
    "- Resume scoring: Compare candidate skills with job requirements using your reasoning\n",
    "- Cover letters: Generate personalized cover letters using your language abilities\n",
    "\n",
    "Be helpful, encouraging, and provide actionable advice.\"\"\"\n",
    "\n",
    "# Create agent - pass Firecrawl tools directly\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=firecrawl_tools,\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "print(\"Agent created successfully!\")\n",
    "print(f\"Tools available: {len(firecrawl_tools)}\")\n",
    "\n",
    "# Quick test (MCP tools require async)\n",
    "test_response = await agent.ainvoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What can you help me with?\"}]\n",
    "})\n",
    "\n",
    "print(\"\\nAgent test response:\")\n",
    "print(test_response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cjhw9smoca",
   "metadata": {},
   "source": [
    "## Live Demos\n",
    "\n",
    "Now let's see the agent in action with progressively complex queries.\n",
    "\n",
    "### Demo 1: Simple Job Search\n",
    "\n",
    "Query: \"Find Python developer jobs in Bangalore\"\n",
    "\n",
    "**Expected Agent Behavior:**\n",
    "1. Recognizes need to scrape job site\n",
    "2. Constructs URL: `https://www.naukri.com/python-developer-jobs-in-bangalore`\n",
    "3. Uses `firecrawl_scrape` tool\n",
    "4. Analyzes and presents results\n",
    "\n",
    "**This will scrape real, live job data from Naukri.com using Firecrawl MCP!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "o4dxg7ozm9j",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent Response:\n",
      "Here are the top 5 Python developer job listings in Bangalore:\n",
      "\n",
      "1. [57,564 Python Job Vacancies In Bangalore - Bengaluru - Naukri.com](https://www.naukri.com/python-jobs-in-bangalore)\n",
      "    - Python jobs in Bangalore ; Python Software Developer. HCLTech · Hybrid - Hyderabad, Bengaluru ; Python Software Developer - ENG @Infosys - PAN INDIA.\n",
      "\n",
      "2. [Python Developer jobs in Bengaluru, Karnataka - Indeed](https://in.indeed.com/q-python-developer-l-bengaluru,-karnataka-jobs.html)\n",
      "    - 3282 Python Developer jobs available in Bengaluru, Karnataka on Indeed.com.\n",
      "\n",
      "3. [2,000+ Python Developer jobs in Bengaluru, Karnataka, India (196 ...](https://in.linkedin.com/jobs/python-developer-jobs-bengaluru)\n",
      "    - Get notified about new Python Developer jobs in Bengaluru, Karnataka, India. Sign in to create job alert. 2,000+ Python Developer Jobs in Bengaluru, Karnataka, ...\n",
      "\n",
      "4. [Python Developer - Caterpillar Careers](https://careers.caterpillar.com/en/jobs/r0000317195/python-developer/)\n",
      "    - Python Developer. Location, Bangalore, Karnataka / Chennai, Tamil Nadu, India. Date Posted, Thursday, November 6, 2025. Apply By, Wednesday ...\n",
      "\n",
      "5. [2989 Python developer jobs in Bengaluru - Glassdoor](https://www.glassdoor.co.in/Job/bengaluru-python-developer-jobs-SRCH_IL.0,9_IC2940587_KO10,26.htm)\n",
      "    - What companies are hiring in Bengaluru? The top companies hiring now are Virtusa,\n"
     ]
    }
   ],
   "source": [
    "# Demo 1: Simple job search\n",
    "response = await agent.ainvoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Find Python developer jobs in Bangalore\"}]\n",
    "})\n",
    "\n",
    "print(\"\\nAgent Response:\")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rfpr1pbmam",
   "metadata": {},
   "source": [
    "### Demo 2: Multi-Step Reasoning with Resume Scoring\n",
    "\n",
    "Query: \"Find Python jobs in Bangalore. My skills are Python, FastAPI, and Docker. Which job should I apply to?\"\n",
    "\n",
    "**Expected Agent Behavior:**\n",
    "1. Constructs Naukri.com URL\n",
    "2. Uses `firecrawl_scrape` to get job listings\n",
    "3. **Uses its own reasoning** to compare candidate skills with job requirements\n",
    "4. Recommends best match with justification\n",
    "\n",
    "**Key Insight:** No separate `score_resume` tool needed! The LLM agent is smart enough to compare skills and provide recommendations using its native language understanding.\n",
    "\n",
    "**This demonstrates autonomous multi-step reasoning with real scraped data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0jbvqluca7r",
   "metadata": {},
   "outputs": [
    {
     "ename": "McpError",
     "evalue": "MCP error -32602: Tool 'firecrawl_search' parameter validation failed: sources.0: Invalid input: expected object, received string. Please check the parameter types and values according to the tool's schema.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMcpError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Demo 2: Multi-step reasoning\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m agent.ainvoke({\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\"\"\u001b[39m\u001b[33mFind Python developer jobs in Bangalore. \u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33m    My skills are: Python, FastAPI, Docker. \u001b[39m\n\u001b[32m      5\u001b[39m \u001b[33m    Which job should I apply to and why?\u001b[39m\u001b[33m\"\"\"\u001b[39m}]\n\u001b[32m      6\u001b[39m })\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAgent Response:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(response[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m].content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workdir/langchain-vllm/.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3182\u001b[39m, in \u001b[36mPregel.ainvoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3179\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3180\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3182\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.astream(\n\u001b[32m   3183\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   3184\u001b[39m     config,\n\u001b[32m   3185\u001b[39m     context=context,\n\u001b[32m   3186\u001b[39m     stream_mode=[\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   3187\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3188\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[32m   3189\u001b[39m     print_mode=print_mode,\n\u001b[32m   3190\u001b[39m     output_keys=output_keys,\n\u001b[32m   3191\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   3192\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   3193\u001b[39m     durability=durability,\n\u001b[32m   3194\u001b[39m     **kwargs,\n\u001b[32m   3195\u001b[39m ):\n\u001b[32m   3196\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   3197\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) == \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workdir/langchain-vllm/.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3000\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2998\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2999\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m3000\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   3001\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   3002\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   3003\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   3004\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   3005\u001b[39m ):\n\u001b[32m   3006\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   3007\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   3008\u001b[39m         stream_mode,\n\u001b[32m   3009\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3012\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   3013\u001b[39m     ):\n\u001b[32m   3014\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workdir/langchain-vllm/.venv/lib/python3.13/site-packages/langgraph/pregel/_runner.py:304\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    302\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    305\u001b[39m         t,\n\u001b[32m    306\u001b[39m         retry_policy,\n\u001b[32m    307\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    308\u001b[39m         configurable={\n\u001b[32m    309\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    310\u001b[39m                 _acall,\n\u001b[32m    311\u001b[39m                 weakref.ref(t),\n\u001b[32m    312\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    313\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    314\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    315\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    316\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    317\u001b[39m                 loop=loop,\n\u001b[32m    318\u001b[39m             ),\n\u001b[32m    319\u001b[39m         },\n\u001b[32m    320\u001b[39m     )\n\u001b[32m    321\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workdir/langchain-vllm/.venv/lib/python3.13/site-packages/langgraph/pregel/_retry.py:137\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    139\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workdir/langchain-vllm/.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:705\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    703\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    704\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    706\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    707\u001b[39m         )\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    709\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workdir/langchain-vllm/.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:473\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    471\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workdir/langchain-vllm/.venv/lib/python3.13/site-packages/langgraph/prebuilt/tool_node.py:749\u001b[39m, in \u001b[36mToolNode._afunc\u001b[39m\u001b[34m(self, input, config, runtime)\u001b[39m\n\u001b[32m    747\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m call, tool_runtime \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tool_calls, tool_runtimes, strict=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    748\u001b[39m     coros.append(\u001b[38;5;28mself\u001b[39m._arun_one(call, input_type, tool_runtime))  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m outputs = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*coros)\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._combine_tool_outputs(outputs, input_type)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workdir/langchain-vllm/.venv/lib/python3.13/site-packages/langgraph/prebuilt/tool_node.py:1088\u001b[39m, in \u001b[36mToolNode._arun_one\u001b[39m\u001b[34m(self, call, input_type, tool_runtime)\u001b[39m\n\u001b[32m   1084\u001b[39m config = tool_runtime.config\n\u001b[32m   1086\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._awrap_tool_call \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wrap_tool_call \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1087\u001b[39m     \u001b[38;5;66;03m# No wrapper - execute directly\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1088\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._execute_tool_async(tool_request, input_type, config)\n\u001b[32m   1090\u001b[39m \u001b[38;5;66;03m# Define async execute callable that can be called multiple times\u001b[39;00m\n\u001b[32m   1091\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute\u001b[39m(req: ToolCallRequest) -> ToolMessage | Command:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workdir/langchain-vllm/.venv/lib/python3.13/site-packages/langgraph/prebuilt/tool_node.py:1037\u001b[39m, in \u001b[36mToolNode._execute_tool_async\u001b[39m\u001b[34m(self, request, input_type, config)\u001b[39m\n\u001b[32m   1034\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m   1036\u001b[39m     \u001b[38;5;66;03m# Error is handled - create error ToolMessage\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m     content = \u001b[43m_handle_tool_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflag\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_tool_errors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1038\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ToolMessage(\n\u001b[32m   1039\u001b[39m         content=content,\n\u001b[32m   1040\u001b[39m         name=call[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   1041\u001b[39m         tool_call_id=call[\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   1042\u001b[39m         status=\u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1043\u001b[39m     )\n\u001b[32m   1045\u001b[39m \u001b[38;5;66;03m# Process successful response\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workdir/langchain-vllm/.venv/lib/python3.13/site-packages/langgraph/prebuilt/tool_node.py:401\u001b[39m, in \u001b[36m_handle_tool_error\u001b[39m\u001b[34m(e, flag)\u001b[39m\n\u001b[32m    399\u001b[39m     content = flag\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(flag):\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     content = \u001b[43mflag\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore [assignment, call-arg]\u001b[39;00m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    403\u001b[39m     msg = (\n\u001b[32m    404\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGot unexpected type of `handle_tool_error`. Expected bool, str \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    405\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mor callable. Received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mflag\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    406\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workdir/langchain-vllm/.venv/lib/python3.13/site-packages/langgraph/prebuilt/tool_node.py:358\u001b[39m, in \u001b[36m_default_handle_tool_errors\u001b[39m\u001b[34m(e)\u001b[39m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ToolInvocationError):\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m e.message\n\u001b[32m--> \u001b[39m\u001b[32m358\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workdir/langchain-vllm/.venv/lib/python3.13/site-packages/langgraph/prebuilt/tool_node.py:990\u001b[39m, in \u001b[36mToolNode._execute_tool_async\u001b[39m\u001b[34m(self, request, input_type, config)\u001b[39m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m990\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m tool.ainvoke(call_args, config)\n\u001b[32m    991\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    992\u001b[39m         \u001b[38;5;66;03m# Filter out errors for injected arguments\u001b[39;00m\n\u001b[32m    993\u001b[39m         filtered_errors = _filter_validation_errors(\n\u001b[32m    994\u001b[39m             exc,\n\u001b[32m    995\u001b[39m             \u001b[38;5;28mself\u001b[39m._tool_to_state_args.get(call[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m], {}),\n\u001b[32m    996\u001b[39m             \u001b[38;5;28mself\u001b[39m._tool_to_store_arg.get(call[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]),\n\u001b[32m    997\u001b[39m             \u001b[38;5;28mself\u001b[39m._tool_to_runtime_arg.get(call[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]),\n\u001b[32m    998\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workdir/langchain-vllm/.venv/lib/python3.13/site-packages/langchain_core/tools/structured.py:63\u001b[39m, in \u001b[36mStructuredTool.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.coroutine:\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# If the tool does not implement async, fall back to default implementation\u001b[39;00m\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m run_in_executor(config, \u001b[38;5;28mself\u001b[39m.invoke, \u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workdir/langchain-vllm/.venv/lib/python3.13/site-packages/langchain_core/tools/base.py:608\u001b[39m, in \u001b[36mBaseTool.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    600\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    602\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    605\u001b[39m     **kwargs: Any,\n\u001b[32m    606\u001b[39m ) -> Any:\n\u001b[32m    607\u001b[39m     tool_input, kwargs = _prep_run_args(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m608\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.arun(tool_input, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workdir/langchain-vllm/.venv/lib/python3.13/site-packages/langchain_core/tools/base.py:1030\u001b[39m, in \u001b[36mBaseTool.arun\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m   1028\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_to_raise:\n\u001b[32m   1029\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_tool_error(error_to_raise)\n\u001b[32m-> \u001b[39m\u001b[32m1030\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error_to_raise\n\u001b[32m   1032\u001b[39m output = _format_output(content, artifact, tool_call_id, \u001b[38;5;28mself\u001b[39m.name, status)\n\u001b[32m   1033\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_tool_end(output, color=color, name=\u001b[38;5;28mself\u001b[39m.name, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workdir/langchain-vllm/.venv/lib/python3.13/site-packages/langchain_core/tools/base.py:999\u001b[39m, in \u001b[36mBaseTool.arun\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m    996\u001b[39m         tool_kwargs[config_param] = config\n\u001b[32m    998\u001b[39m     coro = \u001b[38;5;28mself\u001b[39m._arun(*tool_args, **tool_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m999\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m coro_with_context(coro, context)\n\u001b[32m   1000\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.response_format == \u001b[33m\"\u001b[39m\u001b[33mcontent_and_artifact\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1001\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(response) != \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workdir/langchain-vllm/.venv/lib/python3.13/site-packages/langchain_core/tools/structured.py:117\u001b[39m, in \u001b[36mStructuredTool._arun\u001b[39m\u001b[34m(self, config, run_manager, *args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config_param := _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m.coroutine):\n\u001b[32m    116\u001b[39m         kwargs[config_param] = config\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.coroutine(*args, **kwargs)\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# If self.coroutine is None, then this will delegate to the default\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# implementation which is expected to delegate to _run on a separate thread.\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()._arun(\n\u001b[32m    122\u001b[39m     *args, config=config, run_manager=run_manager, **kwargs\n\u001b[32m    123\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workdir/langchain-vllm/.venv/lib/python3.13/site-packages/langchain_mcp_adapters/tools.py:290\u001b[39m, in \u001b[36mconvert_mcp_tool_to_langchain_tool.<locals>.call_tool\u001b[39m\u001b[34m(**arguments)\u001b[39m\n\u001b[32m    282\u001b[39m handler = _build_interceptor_chain(execute_tool, tool_interceptors)\n\u001b[32m    283\u001b[39m request = MCPToolCallRequest(\n\u001b[32m    284\u001b[39m     name=tool.name,\n\u001b[32m    285\u001b[39m     args=arguments,\n\u001b[32m   (...)\u001b[39m\u001b[32m    288\u001b[39m     runtime=runtime,\n\u001b[32m    289\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m call_tool_result = \u001b[38;5;28;01mawait\u001b[39;00m handler(request)\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _convert_call_tool_result(call_tool_result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workdir/langchain-vllm/.venv/lib/python3.13/site-packages/langchain_mcp_adapters/tools.py:271\u001b[39m, in \u001b[36mconvert_mcp_tool_to_langchain_tool.<locals>.call_tool.<locals>.execute_tool\u001b[39m\u001b[34m(request)\u001b[39m\n\u001b[32m    264\u001b[39m     \u001b[38;5;66;03m# Re-raise the exception outside the context manager\u001b[39;00m\n\u001b[32m    265\u001b[39m     \u001b[38;5;66;03m# This is necessary because the context manager may suppress exceptions\u001b[39;00m\n\u001b[32m    266\u001b[39m     \u001b[38;5;66;03m# This change was introduced to work-around an issue in MCP SDK\u001b[39;00m\n\u001b[32m    267\u001b[39m     \u001b[38;5;66;03m# that may suppress exceptions when the client disconnects.\u001b[39;00m\n\u001b[32m    268\u001b[39m     \u001b[38;5;66;03m# If this is causing an issue, with your use case, please file an issue\u001b[39;00m\n\u001b[32m    269\u001b[39m     \u001b[38;5;66;03m# on the langchain-mcp-adapters GitHub repo.\u001b[39;00m\n\u001b[32m    270\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m captured_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m captured_exception\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    273\u001b[39m     call_tool_result = \u001b[38;5;28;01mawait\u001b[39;00m session.call_tool(\n\u001b[32m    274\u001b[39m         tool_name,\n\u001b[32m    275\u001b[39m         tool_args,\n\u001b[32m    276\u001b[39m         progress_callback=mcp_callbacks.progress_callback,\n\u001b[32m    277\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workdir/langchain-vllm/.venv/lib/python3.13/site-packages/langchain_mcp_adapters/tools.py:255\u001b[39m, in \u001b[36mconvert_mcp_tool_to_langchain_tool.<locals>.call_tool.<locals>.execute_tool\u001b[39m\u001b[34m(request)\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m tool_session.initialize()\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m255\u001b[39m     call_tool_result = \u001b[38;5;28;01mawait\u001b[39;00m tool_session.call_tool(\n\u001b[32m    256\u001b[39m         tool_name,\n\u001b[32m    257\u001b[39m         tool_args,\n\u001b[32m    258\u001b[39m         progress_callback=mcp_callbacks.progress_callback,\n\u001b[32m    259\u001b[39m     )\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# noqa: BLE001\u001b[39;00m\n\u001b[32m    261\u001b[39m     \u001b[38;5;66;03m# Capture exception to re-raise outside context manager\u001b[39;00m\n\u001b[32m    262\u001b[39m     captured_exception = e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workdir/langchain-vllm/.venv/lib/python3.13/site-packages/mcp/client/session.py:350\u001b[39m, in \u001b[36mClientSession.call_tool\u001b[39m\u001b[34m(self, name, arguments, read_timeout_seconds, progress_callback, meta)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m meta \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     _meta = types.RequestParams.Meta(**meta)\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.send_request(\n\u001b[32m    351\u001b[39m     types.ClientRequest(\n\u001b[32m    352\u001b[39m         types.CallToolRequest(\n\u001b[32m    353\u001b[39m             params=types.CallToolRequestParams(name=name, arguments=arguments, _meta=_meta),\n\u001b[32m    354\u001b[39m         )\n\u001b[32m    355\u001b[39m     ),\n\u001b[32m    356\u001b[39m     types.CallToolResult,\n\u001b[32m    357\u001b[39m     request_read_timeout_seconds=read_timeout_seconds,\n\u001b[32m    358\u001b[39m     progress_callback=progress_callback,\n\u001b[32m    359\u001b[39m )\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result.isError:\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._validate_tool_result(name, result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workdir/langchain-vllm/.venv/lib/python3.13/site-packages/mcp/shared/session.py:286\u001b[39m, in \u001b[36mBaseSession.send_request\u001b[39m\u001b[34m(self, request, result_type, request_read_timeout_seconds, metadata, progress_callback)\u001b[39m\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m McpError(\n\u001b[32m    275\u001b[39m         ErrorData(\n\u001b[32m    276\u001b[39m             code=httpx.codes.REQUEST_TIMEOUT,\n\u001b[32m   (...)\u001b[39m\u001b[32m    282\u001b[39m         )\n\u001b[32m    283\u001b[39m     )\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response_or_error, JSONRPCError):\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m McpError(response_or_error.error)\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result_type.model_validate(response_or_error.result)\n",
      "\u001b[31mMcpError\u001b[39m: MCP error -32602: Tool 'firecrawl_search' parameter validation failed: sources.0: Invalid input: expected object, received string. Please check the parameter types and values according to the tool's schema.",
      "During task with name 'tools' and id '65dfec79-5e06-aafe-df14-625ee0953c21'"
     ]
    }
   ],
   "source": [
    "# Demo 2: Multi-step reasoning\n",
    "response = await agent.ainvoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"\"\"Find Python developer jobs in Bangalore. \n",
    "    My skills are: Python, FastAPI, Docker. \n",
    "    Which job should I apply to and why?\"\"\"}]\n",
    "})\n",
    "\n",
    "print(\"\\nAgent Response:\")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8d859294f",
   "metadata": {},
   "source": [
    "### Demo 3: Full Autonomous Workflow\n",
    "\n",
    "Query: \"Find the best Python job in Hyderabad for someone with Python and AWS skills, then write a cover letter for it.\"\n",
    "\n",
    "**Expected Agent Behavior:**\n",
    "1. Constructs URL and scrapes Hyderabad job listings\n",
    "2. **Uses its reasoning** to analyze which job best matches Python + AWS skills\n",
    "3. **Uses its language abilities** to generate a personalized cover letter for that specific company\n",
    "\n",
    "**Key Insights:**\n",
    "- No separate tools for resume scoring or cover letter generation needed!\n",
    "- The LLM agent handles all reasoning tasks naturally\n",
    "- Only uses external tools (Firecrawl) when it needs to fetch external data\n",
    "- Everything else is handled by the agent's native intelligence\n",
    "\n",
    "**This demonstrates full autonomous behavior from a single prompt - the agent plans and executes a complete workflow with real data!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35gp0o2bp8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 3: Full autonomous workflow\n",
    "response = await agent.ainvoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"\"\"Find the best Python developer job in Hyderabad for someone \n",
    "    with Python and AWS skills, then write a personalized cover letter for it.\"\"\"}]\n",
    "})\n",
    "\n",
    "print(\"\\nAgent Response:\")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3o4puyz2tvo",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've built a production-ready LangChain v1 agent with proper MCP integration!\n",
    "\n",
    "### What You Built\n",
    "- vLLM-powered agent with LangChain v1\n",
    "- Direct MCP tool integration (Firecrawl)\n",
    "- Autonomous decision-making with multi-step reasoning\n",
    "- Clean, maintainable architecture\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```python\n",
    "# Get MCP tools\n",
    "firecrawl_tools = await mcp_client.get_tools()\n",
    "\n",
    "# Pass directly to agent\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=firecrawl_tools,\n",
    "    system_prompt=\"...\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Key Principles\n",
    "\n",
    "1. **Trust the LLM Agent** - It's smart enough to construct URLs, parse results, and reason about data\n",
    "2. **Tools for External Data** - Use tools to fetch/scrape data from the web or databases\n",
    "3. **Native Reasoning** - Let the LLM handle analysis, comparison, and generation\n",
    "4. **System Prompt as Guide** - Teach the agent patterns through clear instructions\n",
    "\n",
    "### LangChain v1 Features\n",
    "- `create_agent()` - Modern agent creation\n",
    "- Built on LangGraph for streaming and persistence\n",
    "- Message-based API: `agent.invoke({\"messages\": [...]})`\n",
    "- Direct MCP tool integration via `langchain-mcp-adapters`\n",
    "\n",
    "### Extension Ideas\n",
    "\n",
    "**Quick:**\n",
    "- Add more job sites via system prompt URL patterns\n",
    "- Implement streaming with `agent.stream()`\n",
    "- Add conversation memory\n",
    "\n",
    "**Intermediate:**\n",
    "- Parse PDF resumes with LangChain document loaders\n",
    "- Build Streamlit UI with real-time streaming\n",
    "- Integrate email APIs for automated applications\n",
    "\n",
    "**Advanced:**\n",
    "- Multi-agent system with specialized roles\n",
    "- Human-in-the-loop workflow with LangGraph\n",
    "- Application tracking database\n",
    "- Browser automation for authenticated sites\n",
    "\n",
    "### Resources\n",
    "\n",
    "**LangChain v1:**\n",
    "- [Documentation](https://python.langchain.com/)\n",
    "- [Build an Agent Tutorial](https://python.langchain.com/docs/tutorials/agents/)\n",
    "- [Migration Guide](https://docs.langchain.com/oss/python/migrate/langchain-v1)\n",
    "- [LangGraph](https://langchain-ai.github.io/langgraph/)\n",
    "\n",
    "**MCP:**\n",
    "- [Model Context Protocol](https://modelcontextprotocol.io/)\n",
    "- [LangChain MCP Adapters](https://docs.langchain.com/oss/python/langchain/mcp)\n",
    "- [Firecrawl MCP Server](https://github.com/mendableai/firecrawl-mcp)\n",
    "\n",
    "**vLLM:**\n",
    "- [Documentation](https://docs.vllm.ai/)\n",
    "- [Hermes Models](https://huggingface.co/NousResearch)\n",
    "\n",
    "### Production Tips\n",
    "\n",
    "**Streaming:**\n",
    "```python\n",
    "for chunk in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Find jobs\"}]},\n",
    "    stream_mode=\"updates\"\n",
    "):\n",
    "    print(chunk)\n",
    "```\n",
    "\n",
    "**Persistence:**\n",
    "```python\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=firecrawl_tools,\n",
    "    checkpointer=MemorySaver()\n",
    ")\n",
    "```\n",
    "\n",
    "**Error Handling:**\n",
    "```python\n",
    "try:\n",
    "    response = agent.invoke({\"messages\": [...]})\n",
    "except Exception as e:\n",
    "    print(f\"Agent error: {e}\")\n",
    "    # Implement retry logic\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
